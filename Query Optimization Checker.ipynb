{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Query Optimization Checker.ipynb","provenance":[{"file_id":"1hxh0BsjCeZWpt0VkCsOip9BECL4cLsNx","timestamp":1613815080318}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9-final"}},"cells":[{"cell_type":"markdown","metadata":{"id":"q63X10lRn54d"},"source":["<br>\n","<h1><b>Query Optimization Checker </h1>\n","\n","---\n","\n","> <h3> Figure out what queries are driving clicks / impressions but are not found in <b>{{ PAGE SECTION }}</b>? </h3> \n","> <h3> Use 'xpath_selector' variable to define the <b>{{ PAGE SECTION }}</b>\n","</h3> \n","<br>\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"tjAsXNbzn54o"},"source":["#@title Mount Drive to the Notebook { display-mode: \"form\" }\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bf27EDik1oJJ","executionInfo":{"status":"ok","timestamp":1616315916877,"user_tz":420,"elapsed":691,"user":{"displayName":"jason melman","photoUrl":"","userId":"00137920007750380357"}}},"source":["#@title Import API Module from /My Drive/Colab Notebooks/ { display-mode: \"form\" }\n","import sys\n","sys.path.insert(0, \"drive/My Drive/Colab Notebooks/\")\n","from api import *"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUukgyBKn54r","executionInfo":{"status":"ok","timestamp":1616315917082,"user_tz":420,"elapsed":892,"user":{"displayName":"jason melman","photoUrl":"","userId":"00137920007750380357"}}},"source":["#@title Import External Dependencies { display-mode: \"form\" }\n","import pandas\n","from datetime import datetime\n","from bs4 import BeautifulSoup, NavigableString\n","import requests\n","import random\n","import lxml\n","import lxml.html\n","import lxml.etree\n","from lxml.etree import ParseError\n","from lxml.etree import ParserError\n","from urllib.parse import urlparse\n","import os\n","from google.colab import data_table"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77xcjeahn54s"},"source":["<br>\n","\n","**<h1><< Input Variables >></h1>**\n","\n","\n","---\n","\n","> <h3>These are the <b>ONLY CELLS TO EDIT</b> in the Colab File</h3>\n","> <h3>After Adding Input Variables Use 'CTRL + fn9' to start running script (or go to \"Runtime\" > \"Run All\")</h3>\n","\n","<br>"]},{"cell_type":"code","metadata":{"id":"I7Wg9TxhJm_s"},"source":["#@title Set `xpath_selector` to scrape desired page section { run: \"auto\", display-mode: \"form\" }\n","#@markdown (*No Xpath supplied will result in entire page being used in scrape)\n","xpath_selector = \"//title\" #@param {type: \"string\"}\n","#@markdown ---\n","#@markdown ### Xpath Expressions - Quick Examples:\n","\n","#@markdown [Scrape H1s - Static Scrape] - `//h1`  \n","\n","#@markdown [Scrape Titles - Static Scrape] - `//title`\t   \n","\n","#@markdown [General Template - Custom Scrape] - `//htmlElement[@cssSelector='{{ SELECTOR NAME}}']`\n","\n","#@markdown [Example of a Custom Scrape] - `//div[@class=\"entry-content\"]` \n","\n","#@markdown [Grab First `<p>` in Custom Scrape] - `//div[@class=\"entry-content\"]//p[1]`\n","\n","\n","#@markdown ---\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wn2ze3iKn54t"},"source":["#@title Set `colab_path` to match this files path { run: \"auto\", display-mode: \"form\" }\n","colab_path = \"/content/drive/My Drive/Colab Notebooks/\" #@param {type: \"string\"}\n","#@markdown ---\n","#@markdown ### Set `domain_lookup` to match the homepage URL\n","domain_lookup = 'https://www.inseev.com/'  #@param {type: \"string\"}\n","#@markdown ---\n","#@markdown ### Set `gsc_property` to match the GSC UI property name \n","#@markdown (*For <b>\"domain properties\"</b> use format \"sc-domain:domain.com\")  \n","gsc_property = 'https://www.inseev.com/'  #@param {type: \"string\"}\n","#@markdown ---\n","#@markdown ### Set `startdate` and `enddate` to desired analysis range \n","#@markdown (*Recommend no longer than 3 months from present due to shifting query rankings)  \n","startdate = '2021-02-01'  #@param {type: \"date\"}\n","enddate = '2021-03-13'  #@param {type: \"date\"}\n","#@markdown ---\n","#@markdown ### Set `gsc_sorting_field` and `gsc_limit_pages_number` to desired analysis range \n","#@markdown (*Recommend \"impressions_sum\" for new and \"clicks_sum\" for established properties)  \n","gsc_sorting_field = \"clicks_sum\"   #@param ['clicks_sum', 'impressions_sum']\n","gsc_limit_pages_number =   40#@param {type: \"number\"}\n","\n","#@markdown ---\n","#@markdown ### Set `brand_exclusions` with format <b>query1|query2|...</b>  \n","#@markdown (*Must contain something; if no brand input \"xxx\")\n","brand_exclusions = \"inseev|insev\" #@param {type: \"string\"}\n","#@markdown ---\n","#@markdown ### Set impression <b>LESS THAN X EXCLUSION</b> to remove irrelevant queries\n","#@markdown (*Must contain something; if no exclusion add \"0\")\n","impression_exclusion = \"5\" #@param {type: \"string\"}\n","#@markdown ---\n","#@markdown ### Set page <b>INCLUSIONS</b> with format *urlID1|urlID2|...*\n","#@markdown (*Leave blank if no exclusions needed)\n","page_inclusions = \"\" #@param {type: \"string\"}\n","#@markdown ---\n","\n","import os\n","\n","domain_name = domain_lookup.split(\"www.\")[-1].split(\"//\")[-1].split(\".\")[0]\n","date = datetime.today().strftime('%Y-%m-%d')\n","\n","domain_clean = urlparse(\"{}\".format(domain_lookup)).netloc\n","path_step1 = \"{}/{}/\".format(colab_path,domain_clean)\n","path = \"{}/{}/Query Optimizer_{}\".format(colab_path,domain_clean,date)\n","path_rawData = \"{}/{}/Raw Data (Archive)\".format(colab_path,domain_clean,date)\n","date = datetime.today().strftime('%Y-%m-%d')\n","\n","try:\n","    os.mkdir(path_step1)\n","except OSError:\n","    print (\"Creation of the directory %s failed\" % path_step1)\n","else:\n","    print (\"Successfully created the directory %s \" % path_step1)\n","\n","try:\n","    os.mkdir(path_rawData)\n","except OSError:\n","    print (\"Creation of the directory %s failed\" % path_rawData)\n","else:\n","    print (\"Successfully created the directory %s \" % path_rawData)\n","\n","try:\n","    os.mkdir(path)\n","except OSError:\n","    print (\"Creation of the directory %s failed\" % path)\n","else:    print (\"Successfully created the directory %s \" % path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ATvwYruSn54u"},"source":["<br>\n","\n","**<h1><< End of Input Variables >></h1>**\n","\n","\n","---\n","\n","\n","\n","> <h3> No Other Manual Changes Are Needed </h3> \n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"_kY5Zws4EV6o"},"source":["<h2>PART 1: Get GSC Data </h2>"]},{"cell_type":"code","metadata":{"id":"XdEFaIaWx_Oa","executionInfo":{"status":"ok","timestamp":1616315917084,"user_tz":420,"elapsed":882,"user":{"displayName":"jason melman","photoUrl":"","userId":"00137920007750380357"}}},"source":["#@title Import dependencies { display-mode: \"form\" }\n","import pandas\n","import sys\n","sys.path.insert(0, \"drive/My Drive/Colab Notebooks/\")\n","from api import *\n","from datetime import datetime\n","from bs4 import BeautifulSoup, NavigableString\n","import requests\n","import random\n","import lxml\n","import lxml.html\n","import lxml.etree\n","from lxml.etree import ParseError\n","from lxml.etree import ParserError\n","from urllib.parse import urlparse\n","from google.colab import data_table"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRzNdMFvn54u","scrolled":true},"source":["#@title Collect data from Google Search Console { display-mode: \"form\" }\n","domain_name = domain_lookup.split(\"www.\")[-1].split(\"//\")[-1].split(\".\")[0]\n","date = datetime.today().strftime('%Y-%m-%d')\n","\n","# Options: 'date,' 'device,' 'page,' , 'query' and \"country\"\n","dimensions=['page','query']\n","\n","gsc_df = gscservice.get_site_data(\n","    gsc_property,\n","    startdate=startdate,\n","    enddate=enddate,\n","    dimensions=dimensions,\n","    output_fn=\"{}/{}/Raw Data (Archive)/rawData_{}_{}_{}_by_{}.csv\".format(colab_path,domain_clean,domain_name, startdate.replace(\"-\",\"\"), enddate.replace(\"-\",\"\"), '_'.join(dimensions))\n",")\n","\n","# Filter to only non-brand && specified page type (if any)\n","kw_filter = ~gsc_df[\"query\"].str.contains(\"{}\".format(brand_exclusions), case = False, regex=True)\n","gsc_df = gsc_df[kw_filter]\n","filter_pageType = gsc_df[\"page\"].str.contains(\"{}\".format(page_inclusions), case = False, regex=True)\n","gsc_df = gsc_df[filter_pageType]\n","\n","# filter_pageTypeExclude = ~gsc_df[\"page\"].str.contains(\"{}\".format(pageType_exclude_ngram), case = False, regex=True)\n","# gsc_df = gsc_df[filter_pageTypeExclude]\n","\n","\n","# Insert domain & datasetID\n","gsc_df.insert(loc = 2, column = \"domain\", value = domain_lookup)\n","datasetID = \"{}_{}_{}_by_{}\".format(domain_name, startdate.replace(\"-\",\"\"), enddate.replace(\"-\",\"\"), '_'.join(dimensions))\n","gsc_df.insert(loc = 0, column = \"gsc_datasetID\", value = datasetID)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCwDh6Z9n54v"},"source":["# Preview the export from GSC API\n","gsc_df.to_csv(os.path.join(path,\"step1_raw-page-query-data_{}-{}.csv\".format(domain_name,date)),  index=False)\n","data_table.DataTable(gsc_df, include_index=False, num_rows_per_page=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hKIPU6yun54w"},"source":["<h2>PART 2: Aggregate Data </h2>"]},{"cell_type":"code","metadata":{"id":"aNaOGHyVn54w","scrolled":true,"executionInfo":{"status":"ok","timestamp":1616315981707,"user_tz":420,"elapsed":65470,"user":{"displayName":"jason melman","photoUrl":"","userId":"00137920007750380357"}}},"source":["#@title calculate aggregated metrics and rename dataframe columns { display-mode: \"form\" }\n","select_cols = [\"gsc_datasetID\", \"domain\", \"page\", \"query\", \"clicks\", \"impressions\", \"ctr\", \"position\"]\n","gsc_df = gsc_df[select_cols]\n","\n","# if a query terms list has been defined at the beginning\n","# the GSC dataset will be filtered by that list\n","\n","# if len(query_terms_list) > 0:\n","#     gsc_df = gsc_df.loc[gsc_df['query'] in query_terms_list]\n","\n","grouped_gsc = (\n","    gsc_df.groupby([\"gsc_datasetID\",\"domain\",\"page\"])\n","    .agg({\"clicks\": \"sum\",\n","          \"impressions\": \"sum\",\n","          \"ctr\" : \"mean\",\n","          \"position\":[\"size\",\"max\",\"min\",\"mean\"]})\n","    .reset_index()\n","    .pipe(lambda x: x.set_axis([f'{a}' if b == '' else f'{a}_{b}' for a,b in x.columns], axis=1, inplace=False))\n",")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"0S6Kd-kKn54x"},"source":["# Preview dataset aggregated by page level\n","data_table.DataTable(grouped_gsc.head(100), include_index=False, num_rows_per_page=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2c2PMEbn54y","executionInfo":{"status":"ok","timestamp":1616315981723,"user_tz":420,"elapsed":65468,"user":{"displayName":"jason melman","photoUrl":"","userId":"00137920007750380357"}}},"source":["#@title { display-mode: \"form\" }\n","gsc_all_data = (\n","    gsc_df.groupby([\"gsc_datasetID\",\"domain\",\"page\",\"query\"])\n","    .agg({\"clicks\": \"sum\",\n","          \"impressions\": \"sum\",\n","          \"position\":[\"size\",\"max\",\"min\",\"mean\"]})\n","    .reset_index()\n","    .pipe(lambda x: x.set_axis([f'{a}' if b == '' else f'{a}_{b}' for a,b in x.columns], axis=1, inplace=False))\n","    .round(1)\n",")\n","\n","filter_impressionsLessThan = gsc_all_data[\"impressions_sum\"] > int(\"{}\".format(impression_exclusion))\n","gsc_all_data = gsc_all_data[filter_impressionsLessThan]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Baq7XbPAn540"},"source":["# Preview dataset aggregated by page and query level\n","data_table.DataTable(gsc_all_data.head(100), include_index=False, num_rows_per_page=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ve0CDwU2n541"},"source":["<h2> Part 3: Scrape Top Pages & Find Query Occurrences </h2>\n","\n"]},{"cell_type":"code","metadata":{"id":"VPUKXuaCn541","scrolled":true,"executionInfo":{"status":"ok","timestamp":1616315994656,"user_tz":420,"elapsed":78382,"user":{"displayName":"jason melman","photoUrl":"","userId":"00137920007750380357"}}},"source":["#@title { display-mode: \"form\" }\n","### [BELOW AGGREGATES PAGES FOR SCRAPER]\n","# Take the first N pages based on the variable 'gsc_sorting_field'\n","top_n_pages = grouped_gsc.sort_values(by=gsc_sorting_field, ascending=False)\n","top_n_pages = top_n_pages.reset_index()\n","top_n_pages = top_n_pages.head(gsc_limit_pages_number)[[\"gsc_datasetID\",\"domain\",\"page\"]]\n","# Filter the main dataframe by keeping only data from the top N pages defined above\n","joined_df = gsc_all_data.merge(top_n_pages, on=[\"gsc_datasetID\",\"domain\",\"page\"])\n","\n","def GET_UA():\n","  uastrings = [\n","        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\n","        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.72 Safari/537.36\",\n","        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25\",\n","        \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\n","        \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\n","        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\n","        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.1.17 (KHTML, like Gecko) Version/7.1 Safari/537.85.10\",\n","        \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\",\n","        \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\n","        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36\"\n","  ]\n","  return random.choice(uastrings)\n","crawl_res = []\n","\n","# for each page in the top N list we are going to scrape the html content\n","for index, row in top_n_pages.iterrows():\n","  try:\n","    # 'filtered_df' contains 'joined_df' data sliced \n","    # by the page analyzed during this iteration \n","    filtered_df = joined_df.loc[joined_df['page'] == row.to_dict()['page']]\n","    USER_AGENT = GET_UA()\n","    headers = {'user-agent': USER_AGENT}\n","    resp = requests.get(row.to_dict()['page'], headers=headers)\n","    # parse the HTML with Beautiful soup\n","    if resp.status_code == 200:\n","        if xpath_selector is not None and xpath_selector != '':\n","            tree = lxml.html.fromstring(resp.content)\n","            # Get element using XPath \n","            xpath_selection_content = tree.xpath(xpath_selector)\n","            selected_content = b'\\n'.join([lxml.etree.tostring(elem) for elem in xpath_selection_content])\n","            bs = BeautifulSoup(selected_content, \"html.parser\")\n","        else:\n","            bs = BeautifulSoup(resp.content, \"html.parser\")\n","\n","        # for each query term in the filtered dataframe we find it \n","        # in the parsed html code and we count the occurrences\n","        for i, r in filtered_df.iterrows():\n","            search_query = r.to_dict()['query']\n","            search_domain = r.to_dict()['domain']\n","            search_page = r.to_dict()['page']\n","            search_gsc_dataset_id = r.to_dict()['gsc_datasetID']\n","            # find all occurrences for the query term \n","            # in the Beautifulsoup parsed page content\n","            occ = bs.find_all(text=lambda x: x and search_query in x.lower())\n","            occ_text = []\n","            # if we have found at least one occurrence of the query term \n","            # we just check the Beautifulsoup item class to manage correctly\n","            # the text where each occurrence has been found\n","            if len(occ) > 0:\n","                for o in occ:\n","                    if isinstance(o, NavigableString):\n","                        occ_text.append(str(o))\n","                    else:\n","                        occ_text.append(o.text)\n","\n","                crawl_res.append(dict(\n","                    query=search_query,  # query term we have found in the page content\n","                    domain=search_domain,  # domain from gsc\n","                    page_crawl=search_page,  # page from gsc we have scraped\n","                    gsc_datasetID=search_gsc_dataset_id,  # dataset id from gsc\n","                    text = occ_text,  # list of texts where we have found the query term\n","                    occurrences = len(occ_text)  # number of occurrences for the query term\n","                ))\n","\n","  # below exceptions stop errors from breaking tool\n","  except ParserError as pe:\n","    print(\"ParserError: Error Message - {0}\".format(pe))\n","    pass\n","\n","  except BaseException as ge:\n","    print(\"Unidentified Error - {0}\".format(ge))\n","    pass\n","\n","\n","# transform the list of dictionaries into a dataframe to be able to work with the exisiting dataframes\n","crawl_df = pandas.DataFrame([c for c in crawl_res])"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BAAbWNXn54-"},"source":["# Preview crawled data\n","crawl_df.to_csv(os.path.join(path,\"step2_query-matches_{}-{}.csv\".format(domain_name,date)),  index=False)\n","data_table.DataTable(crawl_df, include_index=False, num_rows_per_page=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kc2M9cWpn55G","scrolled":true},"source":["<h2>Part 4: Merge & Prepare Data </h2>"]},{"cell_type":"code","metadata":{"id":"Ny0LGIlon55H","executionInfo":{"status":"ok","timestamp":1616315994657,"user_tz":420,"elapsed":78365,"user":{"displayName":"jason melman","photoUrl":"","userId":"00137920007750380357"}}},"source":["#@title { display-mode: \"form\" }\n","\n","output = joined_df.merge(crawl_df, how=\"left\", left_on=[\"gsc_datasetID\", \"domain\",\"page\",\"query\"], right_on=[\"gsc_datasetID\", \"domain\",\"page_crawl\",\"query\"])\n","\n","select_cols_final = [\"gsc_datasetID\",\"domain\",\"page\",\"query\",\"clicks_sum\",\"impressions_sum\", \n","        \"position_size\",\"position_max\",\"position_min\",\"position_mean\",\"text\",\"occurrences\"]\n","\n","output = output[select_cols_final]\n","output = output.rename(\n","    mapper={\n","        \"gsc_datasetID\": \"gsc_datasetID\",\n","        \"domain\": \"domain\",\n","        \"page\": \"page_gsc\",\n","        \"query\": \"query_gsc\",\n","        \"clicks_sum\": \"clicks_sum_gsc\",\n","        \"impressions_sum\": \"impressions_sum_gsc\",\n","        \"ctr_mean\": \"ctr_mean_gsc\",\n","        \"position_size\": \"count_instances_gsc\",\n","        \"position_max\": \"position_max_gsc\",\n","        \"position_min\": \"position_min_gsc\",\n","        \"position_mean\": \"position_mean_gsc\",\n","        \"text\": \"text_crawl\",\n","        \"occurrences\": \"occurrences_crawl\"\n","    }, axis=\"columns\")\n","\n","output.text_crawl = output.text_crawl.fillna(value = \"N/A\")\n","output.occurrences_crawl = output.occurrences_crawl.fillna(value = \"No Matching Text - Potential Optimization Opp\")\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEOwjGpGn55K"},"source":["output.to_csv(os.path.join(path,\"step3_query-optimizer_{}-{}.csv\".format(domain_name,date)),  index=False)\n","data_table.DataTable(output, include_index=False, num_rows_per_page=10)"],"execution_count":null,"outputs":[]}]}